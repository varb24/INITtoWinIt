import tensorflow as tf
from tensorflow.keras.applications import EfficientNetB1
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.utils import to_categorical


# Load the EfficientNetB1 pre-trained model (weights from ImageNet)
base_model = EfficientNetB1(include_top=False, weights='imagenet', input_shape=(112, 112, 3))

# Add a GlobalAveragePooling2D layer to reduce the spatial dimensions of the output
x = base_model.output
x = GlobalAveragePooling2D()(x)
num_classes = 100

# Add a fully connected layer with 128 neurons and ReLU activation
x = Dense(128, activation='relu')(x)

# Add a final fully connected layer with softmax activation for classification
predictions = Dense(num_classes, activation='softmax')(x)

# Combine the base model and the fully connected layers into a single model
model = Model(inputs=base_model.input, outputs=predictions)

# Freeze the weights of the base model layers to prevent overfitting
for layer in base_model.layers:
    layer.trainable = False

# Compile the model with categorical cross-entropy loss and Adam optimizer
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
# Define some hyperparameters
batch_size = 32
img_height = 112
img_width = 112
epochs = 10

# Define the ImageDataGenerator with some data augmentation
train_datagen = ImageDataGenerator(rescale=1./255,
                                   rotation_range=20,
                                   width_shift_range=0.2,
                                   height_shift_range=0.2,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True,
                                   fill_mode='nearest')

val_datagen = ImageDataGenerator(rescale=1./255)

# Load the training data
train_generator = train_datagen.flow_from_directory(
        r'C:\Users\jboro\Desktop\INITTOWINIT\TRAINING_DATA',
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode='categorical')


# Load the validation data
validation_generator = val_datagen.flow_from_directory(
        r'C:\Users\jboro\Desktop\INITTOWINIT\VALIDATION_DATA',
        target_size=(img_height, img_width),
        batch_size=batch_size,
        class_mode='categorical')


# Train the model
history = model.fit(
        train_generator,
        steps_per_epoch=train_generator.samples // batch_size,
        epochs=epochs,
        validation_data=validation_generator,
        validation_steps=validation_generator.samples // batch_size)
